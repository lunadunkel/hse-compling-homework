{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc8290",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a5ace",
   "metadata": {},
   "source": [
    "Посчитайте частоты для 5-грамм в корпусе lenta.txt. двумя способами:  \n",
    "1) lenta.txt -> sent_tokenize (russian) -> word_tokenize -> ngrammer  \n",
    "2) lenta.txt -> word_tokenize(preserve_line=True) -> ngrammer  \n",
    "    \n",
    "Проанализируйте топ-20 самых частотных нграмм и проверьте есть ли различия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f5a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18edca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dariasavina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/dariasavina/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957f5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'lenta.txt'\n",
    "\n",
    "with open(path, 'r', encoding='utf8') as file:\n",
    "    lenta = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f21b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(lenta, language='russian')\n",
    "tokenization = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "tokenization = [[token.lower() for token in sentence if re.match(r'\\w+', token)] \n",
    "                       for sentence in tokenization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915cde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c72c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_counts = Counter()\n",
    "for sentence in tokenization:\n",
    "    five_counts.update(ngrammer([token for token in sentence], n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fcfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_five_gramms = sorted(five_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5298bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenization = [x.lower() for x in word_tokenize(lenta, preserve_line=True) if re.match(r'\\w+', x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca90915",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_five_gramms = Counter()\n",
    "second_five_gramms.update(ngrammer(new_tokenization, n=5))\n",
    "second_five_gramms = sorted(second_five_gramms.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96c36dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Общая 5-грамма: риа новости со ссылкой на\n",
      "Количество 5-граммы: 400 в обоих подходах\n",
      "\n",
      "2) Общая 5-грамма: сообщает риа новости со ссылкой\n",
      "Количество 5-граммы: 320 в обоих подходах\n",
      "\n",
      "3) Общая 5-грамма: как сообщили риа новости в\n",
      "Количество 5-граммы: 196 в обоих подходах\n",
      "\n",
      "4) Общая 5-грамма: как сообщает риа новости со\n",
      "Количество 5-граммы: 149 в обоих подходах\n",
      "\n",
      "5) Общая 5-грамма: сообщает интерфакс со ссылкой на\n",
      "Количество 5-граммы: 142 в обоих подходах\n",
      "\n",
      "6) Общая 5-грамма: сообщает итар-тасс со ссылкой на\n",
      "Количество 5-граммы: 118 в обоих подходах\n",
      "\n",
      "7) Общая 5-грамма: об этом риа новости сообщили\n",
      "Количество 5-граммы: 113 в обоих подходах\n",
      "\n",
      "8) Общая 5-грамма: об этом сообщает риа новости\n",
      "Количество 5-граммы: 104 в обоих подходах\n",
      "\n",
      "9) Общая 5-грамма: этом риа новости сообщили в\n",
      "Количество 5-граммы: 99 в обоих подходах\n",
      "\n",
      "10) Общая 5-грамма: со ссылкой на источники в\n",
      "Количество 5-граммы: 93 в обоих подходах\n",
      "\n",
      "11) Общая 5-грамма: сообщили риа новости в пресс-службе\n",
      "Количество 5-граммы: 88 в обоих подходах\n",
      "\n",
      "12) 5-грамма не совпадает: \n",
      "группировки войск на северном кавказе vs как сообщает интерфакс со ссылкой\n",
      "84 vs 83\n",
      "\n",
      "13) 5-грамма не совпадает: \n",
      "как сообщает интерфакс со ссылкой vs объединенной группировки войск на северном\n",
      "83 vs 83\n",
      "\n",
      "14) 5-грамма не совпадает: \n",
      "объединенной группировки войск на северном vs эхо москвы со ссылкой на\n",
      "83 vs 77\n",
      "\n",
      "15) Общая 5-грамма: новости со ссылкой на пресс-службу\n",
      "Количество 5-граммы: 76 в обоих подходах\n",
      "\n",
      "16) 5-грамма не совпадает: \n",
      "эхо москвы со ссылкой на vs этом сообщает риа новости со\n",
      "76 vs 75\n",
      "\n",
      "17) 5-грамма не совпадает: \n",
      "этом сообщает риа новости со vs в связи с тем что\n",
      "75 vs 70\n",
      "\n",
      "18) 5-грамма не совпадает: \n",
      "в связи с тем что vs как сообщает итар-тасс со ссылкой\n",
      "70 vs 58\n",
      "\n",
      "19) 5-грамма не совпадает: \n",
      "по борьбе с организованной преступностью vs группировки войск на северном кавказе\n",
      "66 vs 57\n",
      "\n",
      "20) 5-грамма не совпадает: \n",
      "как сообщает итар-тасс со ссылкой vs по борьбе с организованной преступностью\n",
      "58 vs 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, (f, s) in enumerate(zip(first_five_gramms[:20], second_five_gramms[:20])):\n",
    "    if f[0] == s[0]:\n",
    "        print(f'{num+1}) Общая 5-грамма: {f[0]}')\n",
    "        if f[1] == s[1]:\n",
    "            print(f'Количество 5-граммы: {f[1]} в обоих подходах')\n",
    "        elif f[1] > s[1]:\n",
    "            print(f'В первом подходе этой 5-граммы больше на {f[1]-s[1]} (всего {f[1]} и {s[1]})')\n",
    "        elif f[1] < s[1]:\n",
    "            print(f'Во втором подходе этой 5-граммы больше на {s[1]-f[1]} (всего {f[1]} и {s[1]})')\n",
    "    else:\n",
    "        print(f'{num+1}) 5-грамма не совпадает: \\n{f[0]} vs {s[0]}\\n{f[1]} vs {s[1]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f603fcc",
   "metadata": {},
   "source": [
    "первые 11 5-грамм совпадают"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8d8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'как славно сейчас было бы съесть эти чудесные пряники. жаль, что они на новый год'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e3c53da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['как славно сейчас было бы',\n",
       " 'славно сейчас было бы съесть',\n",
       " 'сейчас было бы съесть эти',\n",
       " 'было бы съесть эти чудесные',\n",
       " 'бы съесть эти чудесные пряники.',\n",
       " 'съесть эти чудесные пряники. жаль',\n",
       " 'эти чудесные пряники. жаль ,',\n",
       " 'чудесные пряники. жаль , что',\n",
       " 'пряники. жаль , что они',\n",
       " 'жаль , что они на',\n",
       " ', что они на новый',\n",
       " 'что они на новый год']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrammer(word_tokenize(text, preserve_line=True), n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96feafc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['как славно сейчас было бы',\n",
       " 'славно сейчас было бы съесть',\n",
       " 'сейчас было бы съесть эти',\n",
       " 'было бы съесть эти чудесные',\n",
       " 'бы съесть эти чудесные пряники',\n",
       " 'съесть эти чудесные пряники .',\n",
       " 'жаль , что они на',\n",
       " ', что они на новый',\n",
       " 'что они на новый год']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = []\n",
    "for x in [word_tokenize(x) for x in sent_tokenize(text, language='russian')]:\n",
    "    all.extend(ngrammer(x, n=5))\n",
    "all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359be5f2",
   "metadata": {},
   "source": [
    "Засчет параметра preserve_line у нас получается токенизация происходит как будто бы одной целой строчки. То есть на стыках предложений тоже будет образовываться n-граммы (пример выше для наглядности без очистки от пунктуации)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781f34",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292716e",
   "metadata": {},
   "source": [
    "Найдите какую-то инетересную (по вашему мнению) закономерность на https://books.google.com/ngrams/ для русского языка (с 1990 по 2022)\n",
    "\n",
    "Вставьте сюда скриншот"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bb213",
   "metadata": {},
   "source": [
    "Все меньше токсичности и все больше осознанности :) и они практически взаимоисключаемы!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483e823",
   "metadata": {},
   "source": [
    "<iframe name=\"ngram_chart\" src=\"https://books.google.com/ngrams/interactive_chart?content=токсичность,осознанность&year_start=1990&year_end=2022&corpus=ru&smoothing=3\" width=900 height=500 marginwidth=0 marginheight=0 hspace=0 vspace=0 frameborder=0 scrolling=no></iframe>\n"
   ]
  },
  {
   "attachments": {
    "осознанность core": {
     "image/png": "осознанность core.png"
    }
   },
   "cell_type": "markdown",
   "id": "e71a72f4",
   "metadata": {},
   "source": [
    "![Снимок экрана 2025-09-27 в 01.29.30.png](<attachment:Снимок экрана 2025-09-27 в 01.29.30.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a89ec",
   "metadata": {},
   "source": [
    "## Заданиe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c35e9",
   "metadata": {},
   "source": [
    "Когда мы разбирали PMI мы использовали такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221f1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd2def",
   "metadata": {},
   "source": [
    "Но если вы посмотрите на определение в википедии, то увидите, что формула немного другая ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/094243d23c19d2d032f6bb26c4dc4f47d98d32f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1905862",
   "metadata": {},
   "source": [
    "Перепишите функцию, чтобы она точно соответствовала этому определению. Расчитайте PMI для всех биграммов также как мы делали в семинаре с помощью функции score_bigrams используя изначальный scorer и обновленный. Посмотрите есть ли разница в топ-10 биграммов. Подумайте почему результаты совпадают/отличаются?\n",
    "\n",
    "*Подсказка: для вероятностей можно поделить на количество слов в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e0622ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99901522",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm_counts = Counter()\n",
    "for sentence in tokenization:\n",
    "    bigramm_counts.update(ngrammer([token for token in sentence], n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b731d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for x in tokenization:\n",
    "    word_counter.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1431f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(word_count_x, word_count_y, bigram_count, corpus_length, all_bigrams_count):\n",
    "    p_x = word_count_x / corpus_length\n",
    "    p_y = word_count_y / corpus_length\n",
    "    p_x_y = bigram_count / all_bigrams_count\n",
    "    try:\n",
    "        pmi_score = math.log((p_x_y / (p_x * p_y)), 2)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return pmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3193226",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_scores = {}\n",
    "new_scores = {}\n",
    "\n",
    "all_bigramms_count = sum(bigramm_counts.values())\n",
    "corpus_length = sum(word_counter.values())\n",
    "\n",
    "for bigram in bigramm_counts.items():\n",
    "    word_x, word_y = bigram[0].split()[0], bigram[0].split()[1]\n",
    "    word_count_x = word_counter[word_x]\n",
    "    word_count_y = word_counter[word_y]\n",
    "    bigram_count = bigram[1]\n",
    "    simple_scores[bigram[0]] = scorer_simple(word_count_x, word_count_y, bigram_count)\n",
    "    new_scores[bigram[0]] = score_bigrams(word_count_x, word_count_y, bigram_count, corpus_length, all_bigramms_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e010bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('неприятель приблизившись', 0.5),\n",
       " ('саноку обстреливалась', 0.5),\n",
       " ('м.ю лермонтова', 0.5),\n",
       " ('австрийский аэроплан', 0.5),\n",
       " ('показывался аэроплан-птица', 0.5),\n",
       " ('das ist', 0.5),\n",
       " ('ist nesteroff', 0.5),\n",
       " ('могучий унесся', 0.5),\n",
       " ('зловеще гремели.и', 0.5),\n",
       " ('гремели.и пламенно', 0.5)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(simple_scores.items(), key=lambda x: -x[1]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a95d1944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('бои у', 3.9069935108378138),\n",
       " ('у сопоцкина', 9.299310933616574),\n",
       " ('сопоцкина и', 5.579140311374476),\n",
       " ('и друскеник', 5.579140311374476),\n",
       " ('друскеник закончились', 15.941434849770062),\n",
       " ('закончились отступлением', 14.941434849770062),\n",
       " ('отступлением германцев', 19.585291039544785),\n",
       " ('неприятель приблизившись', 20.585291039544785),\n",
       " ('приблизившись с', 6.643418118999718),\n",
       " ('с севера', 4.406378921698869)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_scores.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad771e2",
   "metadata": {},
   "source": [
    "Если я правильно понимаю, то, во-первых, у нас разная нормализация:\n",
    "- в более простом случае мы просто делим на сумму вхождения двух слов из би-граммы\n",
    "- в PMI метрики мы учитываем размерности корпуса слов и корпуса би-грамм\n",
    "\n",
    "Также у нас отличается область значений:\n",
    "- в обычном скорере у нас максимальное значение достигается, когда количество слов $a$ совпадает с количеством слов $b$ и они всегда составляют би-грамму. тогда при $bi_{ab} = min(a, b) = a | a = b$ мы получаем, что значением выражения после подставновки (например, $a$) равняется $a / 2a = 1/2$. Соответственно, $score \\in [0; 0.5] $ \n",
    "- у PMI нет ограничений в области значений, но одновременно, кажется, он сильно чувствителен (несмотря на логарифм) к редким и частым вхождениям слов при их большом и малом со-вхождении соответственно. То есть у нас метрика очень зависит от распределению слов в корпусе; метрика как бы штрафует, если слова часто встречаются раздельно, но редко вместе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e1c99",
   "metadata": {},
   "source": [
    "## Задание 4*\n",
    "\n",
    "Обновите функцию получившуюся в предыдущем задании так, чтобы вместо произведения/деления вероятностей использовались сложение и вычитание логирифмов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f55a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logaripmic_score_bigrams(word_count_x, word_count_y, bigram_count, corpus_length, all_bigrams_count):\n",
    "    p_x = word_count_x / corpus_length\n",
    "    p_y = word_count_y / corpus_length\n",
    "    p_x_y = bigram_count / all_bigrams_count\n",
    "    try:\n",
    "        pmi_score = math.log(p_x_y, 2) - math.log(p_x, 2) - math.log(p_y, 2)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return pmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf0c5e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бои у 3.906993510837818\n",
      "у сопоцкина 9.299310933616576\n",
      "сопоцкина и 5.579140311374478\n",
      "и друскеник 5.579140311374477\n",
      "друскеник закончились 15.941434849770065\n",
      "закончились отступлением 14.941434849770065\n",
      "отступлением германцев 19.58529103954479\n",
      "неприятель приблизившись 20.58529103954479\n",
      "приблизившись с 6.64341811899972\n",
      "севера к 2.9674864146787616\n"
     ]
    }
   ],
   "source": [
    "for bigram in sorted(bigramm_counts.items(), key=lambda x: x[1])[:10]:\n",
    "    word_x, word_y = bigram[0].split()[0], bigram[0].split()[1]\n",
    "    word_count_x = word_counter[word_x]\n",
    "    word_count_y = word_counter[word_y]\n",
    "    bigram_count = bigram[1]\n",
    "    print(bigram[0], logaripmic_score_bigrams(word_count_x, word_count_y, bigram_count, corpus_length, all_bigramms_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
